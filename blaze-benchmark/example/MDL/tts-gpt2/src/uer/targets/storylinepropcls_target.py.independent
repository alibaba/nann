# -*- encoding:utf-8 -*-
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from uer.layers.layer_norm import LayerNorm
from uer.utils.act_fun import gelu


class StorylinepropclsTarget(nn.Module):
    """
    """
    def __init__(self, args, vocab_size):
        super(StorylinepropclsTarget, self).__init__()
        self.vocab_size = vocab_size
        self.hidden_size = args.hidden_size

        #self.output_layer = nn.Linear(self.hidden_size * 2, 1)
        self.criterion = nn.BCEWithLogitsLoss(reduction='none')
        self.sigmoid = nn.Sigmoid()


    def forward(self, memory_bank, target_words_hidden, target_mask):
        """
        Args:
            memory_bank: [batch_size x seq_length x hidden_size]
            tgt: [batch_size x seq_length]

        Returns:
            loss: Language modeling loss.
            correct: Number of words that are predicted correctly.
            denominator: Number of predicted words.
        """

        batch_size, target_words_num, hidden_size = target_words_hidden.size()

        '''
        memory_bank = memory_bank.unsqueeze(1).repeat(1, batch_size, target_words_num, 1)
        target_words_hidden = target_words_hidden.unsqueeze(0).repeat(batch_size, 1, 1, 1)

        memory_bank = torch.cat((memory_bank, target_words_hidden), -1)
        output = self.output_layer(memory_bank).squeeze(-1)

        target = torch.eye(batch_size, device=memory_bank.device).unsqueeze(-1).repeat(1, 1, target_words_num)

        loss = self.criterion(output, target)

        target_mask = target_mask.unsqueeze(0).repeat(batch_size, 1, 1)
        loss *= target_mask
        denominator = target_mask.sum()
        loss = loss.sum() / denominator

        prob = self.sigmoid(output)

        correct = torch.sum(target_mask * ((prob > 0.5).int().eq(target.int())).float())
        '''

        '''
        memory_bank = memory_bank.unsqueeze(1).repeat(1, 2, target_words_num, 1)
        index = list(range(batch_size))
        import random
        random.shuffle(index)
        neg_words_hidden = target_words_hidden[index, :, :]
        target_words_hidden = torch.cat((target_words_hidden.unsqueeze(1), neg_words_hidden.unsqueeze(1)), 1)

        memory_bank = torch.cat((memory_bank, target_words_hidden), -1)
        output = self.output_layer(memory_bank).squeeze(-1)

        target = torch.cat((torch.ones(batch_size, 1, target_words_num, device=memory_bank.device), torch.zeros(batch_size, 1, target_words_num, device=memory_bank.device)), 1)

        loss = self.criterion(output, target)

        target_mask = torch.cat((target_mask.unsqueeze(1), target_mask[index, :].unsqueeze(1)), 1)
        loss *= target_mask
        denominator = target_mask.sum()
        loss = loss.sum() / denominator
        loss *= 10

        prob = self.sigmoid(output)

        correct = torch.sum(target_mask * ((prob > 0.5).int().eq(target.int())).float())
        '''

        '''
        memory_bank = memory_bank.unsqueeze(1).repeat(1, 2, target_words_num, 1)
        index = list(range(batch_size))
        import random
        random.shuffle(index)
        neg_words_hidden = target_words_hidden[index, :, :]
        target_words_hidden = torch.cat((target_words_hidden.unsqueeze(1), neg_words_hidden.unsqueeze(1)), 1)

        output = (memory_bank * target_words_hidden).sum(-1)

        target = torch.cat((torch.ones(batch_size, 1, target_words_num, device=memory_bank.device), torch.zeros(batch_size, 1, target_words_num, device=memory_bank.device)), 1)

        loss = self.criterion(output, target)

        target_mask = torch.cat((target_mask.unsqueeze(1), target_mask[index, :].unsqueeze(1)), 1)
        loss *= target_mask
        denominator = target_mask.sum()
        loss = loss.sum() / denominator

        prob = self.sigmoid(output)

        correct = torch.sum(target_mask * ((prob > 0.5).int().eq(target.int())).float())
        '''

        memory_bank = memory_bank.repeat(1, target_words_num, 1)
        index = list(range(batch_size))
        import random
        random.shuffle(index)
        neg_words_hidden = target_words_hidden[index, :, :]

        pos_output = F.cosine_similarity(memory_bank, target_words_hidden, dim=-1)
        neg_output = F.cosine_similarity(memory_bank, neg_words_hidden, dim=-1)

        target = torch.ones(batch_size, target_words_num, device=memory_bank.device)

        output = (pos_output - neg_output) * 5
        loss = self.criterion(output, target)

        loss *= target_mask
        denominator = target_mask.sum()
        loss = loss.sum() / denominator

        prob = self.sigmoid(output)

        correct = torch.sum(target_mask * ((prob > 0.5).int().eq(target.int())).float())

        return loss, correct, denominator
