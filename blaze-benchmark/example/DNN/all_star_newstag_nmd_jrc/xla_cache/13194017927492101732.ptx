//
// Generated by LLVM NVPTX Back-End
//

.version 6.3
.target sm_75
.address_size 64

	// .globl	reduce_9
.visible .global .align 64 .b8 buffer_for_constant_4[4] = {0, 0, 128, 255};
.visible .global .align 64 .b8 buffer_for_constant_14[4];

.visible .entry reduce_9(
	.param .u64 reduce_9_param_0,
	.param .u64 reduce_9_param_1,
	.param .u64 reduce_9_param_2
)
.reqntid 32, 1, 1
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<29>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<18>;

	ld.param.u64 	%rd4, [reduce_9_param_1];
	cvta.to.global.u64 	%rd5, %rd4;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r3, %r2, 200;
	add.s32 	%r4, %r3, %r1;
	mul.wide.u32 	%rd6, %r4, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.f32 	%f6, [%rd7];
	max.f32 	%f7, %f6, 0fFF800000;
	cvt.u64.u32 	%rd8, %r3;
	cvt.u64.u32 	%rd9, %r1;
	add.s64 	%rd10, %rd8, %rd9;
	shl.b64 	%rd11, %rd10, 2;
	add.s64 	%rd2, %rd5, %rd11;
	ld.global.nc.f32 	%f8, [%rd2+128];
	max.f32 	%f9, %f7, %f8;
	ld.global.nc.f32 	%f10, [%rd2+256];
	max.f32 	%f11, %f9, %f10;
	ld.global.nc.f32 	%f12, [%rd2+384];
	max.f32 	%f13, %f11, %f12;
	ld.global.nc.f32 	%f14, [%rd2+512];
	max.f32 	%f15, %f13, %f14;
	ld.global.nc.f32 	%f16, [%rd2+640];
	max.f32 	%f28, %f15, %f16;
	or.b32  	%r5, %r1, 192;
	setp.lt.u32 	%p1, %r5, 200;
	@%p1 bra 	LBB0_1;
	bra.uni 	LBB0_2;
LBB0_1:
	ld.global.nc.f32 	%f17, [%rd2+768];
	max.f32 	%f28, %f28, %f17;
LBB0_2:
	shfl.sync.down.b32 %f18, %f28, 16, 31, -1;
	max.f32 	%f19, %f28, %f18;
	shfl.sync.down.b32 %f20, %f19, 8, 31, -1;
	max.f32 	%f21, %f19, %f20;
	shfl.sync.down.b32 %f22, %f21, 4, 31, -1;
	max.f32 	%f23, %f21, %f22;
	shfl.sync.down.b32 %f24, %f23, 2, 31, -1;
	max.f32 	%f4, %f23, %f24;
	shfl.sync.down.b32 %f5, %f4, 1, 31, -1;
	setp.ne.s32 	%p2, %r1, 0;
	@%p2 bra 	LBB0_4;
	ld.param.u64 	%rd3, [reduce_9_param_0];
	cvta.to.global.u64 	%rd1, %rd3;
	max.f32 	%f25, %f4, %f5;
	mul.wide.u32 	%rd12, %r2, 1374389535;
	shr.u64 	%rd13, %rd12, 38;
	cvt.u32.u64 	%r6, %rd13;
	mul.lo.s32 	%r7, %r6, 200;
	sub.s32 	%r8, %r2, %r7;
	and.b32  	%r9, %r6, 3;
	mul.wide.u32 	%rd14, %r9, 800;
	add.s64 	%rd15, %rd1, %rd14;
	mul.wide.u32 	%rd16, %r8, 4;
	add.s64 	%rd17, %rd15, %rd16;
	ld.global.f32 	%f26, [%rd17];
	max.f32 	%f27, %f26, %f25;
	st.global.f32 	[%rd17], %f27;
LBB0_4:
	ret;

}
	// .globl	fusion_1
.visible .entry fusion_1(
	.param .u64 fusion_1_param_0,
	.param .u64 fusion_1_param_1,
	.param .u64 fusion_1_param_2
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<52>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<14>;

	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %tid.x;
	shl.b32 	%r5, %r3, 8;
	or.b32  	%r1, %r5, %r4;
	setp.lt.u32 	%p1, %r1, 40000;
	@%p1 bra 	LBB1_2;
	bra.uni 	LBB1_1;
LBB1_2:
	ld.param.u64 	%rd4, [fusion_1_param_0];
	ld.param.u64 	%rd5, [fusion_1_param_2];
	cvta.to.global.u64 	%rd1, %rd5;
	ld.param.u64 	%rd6, [fusion_1_param_1];
	cvta.to.global.u64 	%rd2, %rd6;
	cvta.to.global.u64 	%rd3, %rd4;
	shl.b32 	%r2, %r1, 2;
	cvt.u16.u32 	%rs1, %r1;
	shr.u16 	%rs2, %rs1, 4;
	mul.wide.u16 	%r6, %rs2, 839;
	shr.u16 	%rs3, %rs1, 1;
	mul.wide.u16 	%r7, %rs3, 5243;
	shr.u32 	%r8, %r7, 17;
	cvt.u16.u32 	%rs4, %r8;
	shr.u32 	%r9, %r7, 20;
	cvt.u16.u32 	%rs5, %r9;
	mul.wide.u16 	%r10, %rs5, 5243;
	shr.u32 	%r11, %r10, 17;
	cvt.u16.u32 	%rs6, %r11;
	mul.lo.s16 	%rs7, %rs6, 200;
	sub.s16 	%rs8, %rs4, %rs7;
	mul.wide.u32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.nc.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd8];
	bfe.u32 	%r12, %r6, 19, 2;
	mul.wide.u32 	%rd9, %r12, 800;
	add.s64 	%rd10, %rd3, %rd9;
	cvt.u32.u16 	%r13, %rs8;
	mul.wide.u32 	%rd11, %r13, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.nc.f32 	%f5, [%rd12];
	sub.rn.f32 	%f6, %f1, %f5;
	fma.rn.f32 	%f7, %f6, 0f3BBB989D, 0f3F000000;
	cvt.sat.f32.f32 	%f8, %f7;
	mov.f32 	%f9, 0f4B400001;
	mov.f32 	%f10, 0f437C0000;
	fma.rm.f32 	%f11, %f8, %f10, %f9;
	add.rn.f32 	%f12, %f11, 0fCB40007F;
	neg.f32 	%f13, %f12;
	fma.rn.f32 	%f14, %f6, 0f3FB8AA3B, %f13;
	fma.rn.f32 	%f15, %f6, 0f32A57060, %f14;
	mov.b32 	%r14, %f11;
	shl.b32 	%r15, %r14, 23;
	mov.b32 	%f16, %r15;
	ex2.approx.ftz.f32 	%f17, %f15;
	mul.rn.f32 	%f18, %f17, %f16;
	add.s64 	%rd13, %rd1, %rd7;
	sub.rn.f32 	%f19, %f2, %f5;
	fma.rn.f32 	%f20, %f19, 0f3BBB989D, 0f3F000000;
	cvt.sat.f32.f32 	%f21, %f20;
	fma.rm.f32 	%f22, %f21, %f10, %f9;
	add.rn.f32 	%f23, %f22, 0fCB40007F;
	neg.f32 	%f24, %f23;
	fma.rn.f32 	%f25, %f19, 0f3FB8AA3B, %f24;
	fma.rn.f32 	%f26, %f19, 0f32A57060, %f25;
	mov.b32 	%r16, %f22;
	shl.b32 	%r17, %r16, 23;
	mov.b32 	%f27, %r17;
	ex2.approx.ftz.f32 	%f28, %f26;
	mul.rn.f32 	%f29, %f28, %f27;
	sub.rn.f32 	%f30, %f3, %f5;
	fma.rn.f32 	%f31, %f30, 0f3BBB989D, 0f3F000000;
	cvt.sat.f32.f32 	%f32, %f31;
	fma.rm.f32 	%f33, %f32, %f10, %f9;
	add.rn.f32 	%f34, %f33, 0fCB40007F;
	neg.f32 	%f35, %f34;
	fma.rn.f32 	%f36, %f30, 0f3FB8AA3B, %f35;
	fma.rn.f32 	%f37, %f30, 0f32A57060, %f36;
	mov.b32 	%r18, %f33;
	shl.b32 	%r19, %r18, 23;
	mov.b32 	%f38, %r19;
	ex2.approx.ftz.f32 	%f39, %f37;
	mul.rn.f32 	%f40, %f39, %f38;
	sub.rn.f32 	%f41, %f4, %f5;
	fma.rn.f32 	%f42, %f41, 0f3BBB989D, 0f3F000000;
	cvt.sat.f32.f32 	%f43, %f42;
	fma.rm.f32 	%f44, %f43, %f10, %f9;
	add.rn.f32 	%f45, %f44, 0fCB40007F;
	neg.f32 	%f46, %f45;
	fma.rn.f32 	%f47, %f41, 0f3FB8AA3B, %f46;
	fma.rn.f32 	%f48, %f41, 0f32A57060, %f47;
	mov.b32 	%r20, %f44;
	shl.b32 	%r21, %r20, 23;
	mov.b32 	%f49, %r21;
	ex2.approx.ftz.f32 	%f50, %f48;
	mul.rn.f32 	%f51, %f50, %f49;
	st.global.v4.f32 	[%rd13], {%f18, %f29, %f40, %f51};
LBB1_1:
	ret;

}
	// .globl	reduce_19
.visible .entry reduce_19(
	.param .u64 reduce_19_param_0
)
.reqntid 32, 1, 1
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<29>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<17>;

	ld.param.u64 	%rd3, [reduce_19_param_0];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r3, %r2, 200;
	add.s32 	%r4, %r3, %r1;
	mul.wide.u32 	%rd5, %r4, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.f32 	%f6, [%rd6];
	add.rn.f32 	%f7, %f6, 0f00000000;
	cvt.u64.u32 	%rd7, %r3;
	cvt.u64.u32 	%rd8, %r1;
	add.s64 	%rd9, %rd7, %rd8;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd2, %rd4, %rd10;
	ld.global.nc.f32 	%f8, [%rd2+128];
	add.rn.f32 	%f9, %f7, %f8;
	ld.global.nc.f32 	%f10, [%rd2+256];
	add.rn.f32 	%f11, %f9, %f10;
	ld.global.nc.f32 	%f12, [%rd2+384];
	add.rn.f32 	%f13, %f11, %f12;
	ld.global.nc.f32 	%f14, [%rd2+512];
	add.rn.f32 	%f15, %f13, %f14;
	ld.global.nc.f32 	%f16, [%rd2+640];
	add.rn.f32 	%f28, %f15, %f16;
	or.b32  	%r5, %r1, 192;
	setp.lt.u32 	%p1, %r5, 200;
	@%p1 bra 	LBB2_1;
	bra.uni 	LBB2_2;
LBB2_1:
	ld.global.nc.f32 	%f17, [%rd2+768];
	add.rn.f32 	%f28, %f28, %f17;
LBB2_2:
	shfl.sync.down.b32 %f18, %f28, 16, 31, -1;
	add.rn.f32 	%f19, %f28, %f18;
	shfl.sync.down.b32 %f20, %f19, 8, 31, -1;
	add.rn.f32 	%f21, %f19, %f20;
	shfl.sync.down.b32 %f22, %f21, 4, 31, -1;
	add.rn.f32 	%f23, %f21, %f22;
	shfl.sync.down.b32 %f24, %f23, 2, 31, -1;
	add.rn.f32 	%f4, %f23, %f24;
	shfl.sync.down.b32 %f5, %f4, 1, 31, -1;
	setp.ne.s32 	%p2, %r1, 0;
	@%p2 bra 	LBB2_4;
	add.s64 	%rd1, %rd4, 640000;
	add.rn.f32 	%f25, %f4, %f5;
	mul.wide.u32 	%rd11, %r2, 1374389535;
	shr.u64 	%rd12, %rd11, 38;
	cvt.u32.u64 	%r6, %rd12;
	mul.lo.s32 	%r7, %r6, 200;
	sub.s32 	%r8, %r2, %r7;
	and.b32  	%r9, %r6, 3;
	mul.wide.u32 	%rd13, %r9, 800;
	add.s64 	%rd14, %rd1, %rd13;
	mul.wide.u32 	%rd15, %r8, 4;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.f32 	%f26, [%rd16];
	add.rn.f32 	%f27, %f25, %f26;
	st.global.f32 	[%rd16], %f27;
LBB2_4:
	ret;

}
	// .globl	fusion
.visible .entry fusion(
	.param .u64 fusion_param_0,
	.param .u64 fusion_param_1
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<10>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<12>;

	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %tid.x;
	shl.b32 	%r5, %r3, 8;
	or.b32  	%r1, %r5, %r4;
	setp.lt.u32 	%p1, %r1, 40000;
	@%p1 bra 	LBB3_2;
	bra.uni 	LBB3_1;
LBB3_2:
	ld.param.u64 	%rd3, [fusion_param_0];
	ld.param.u64 	%rd4, [fusion_param_1];
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd3;
	shl.b32 	%r2, %r1, 2;
	cvt.u16.u32 	%rs1, %r1;
	shr.u16 	%rs2, %rs1, 4;
	mul.wide.u16 	%r6, %rs2, 839;
	shr.u32 	%r7, %r6, 19;
	mul.wide.u32 	%rd5, %r7, 800;
	add.s64 	%rd6, %rd1, %rd5;
	shr.u16 	%rs3, %rs1, 1;
	mul.wide.u16 	%r8, %rs3, 5243;
	shr.u32 	%r9, %r8, 17;
	cvt.u16.u32 	%rs4, %r9;
	shr.u32 	%r10, %r8, 20;
	cvt.u16.u32 	%rs5, %r10;
	mul.wide.u16 	%r11, %rs5, 5243;
	shr.u32 	%r12, %r11, 17;
	cvt.u16.u32 	%rs6, %r12;
	mul.lo.s16 	%rs7, %rs6, 200;
	sub.s16 	%rs8, %rs4, %rs7;
	mul.wide.u32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd8];
	cvt.u32.u16 	%r13, %rs8;
	mul.wide.u32 	%rd9, %r13, 4;
	add.s64 	%rd10, %rd6, %rd9;
	ld.global.nc.f32 	%f5, [%rd10+640000];
	div.full.f32 	%f6, %f1, %f5;
	add.s64 	%rd11, %rd2, %rd7;
	div.full.f32 	%f7, %f2, %f5;
	div.full.f32 	%f8, %f3, %f5;
	div.full.f32 	%f9, %f4, %f5;
	st.global.v4.f32 	[%rd11], {%f6, %f7, %f8, %f9};
LBB3_1:
	ret;

}

